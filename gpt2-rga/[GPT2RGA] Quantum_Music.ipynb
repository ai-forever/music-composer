{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ac5a4cf0-d9d2-47b5-9633-b53f8d99a4d2",
     "kernelId": ""
    },
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Quantum Music (ver. 1.0)\n",
    "\n",
    "## Tokenized Sparse Time Quantization Example\n",
    "\n",
    "***\n",
    "\n",
    "Powered by tegridy-tools TMIDIX Optimus Processors: https://github.com/asigalov61/tegridy-tools\n",
    "\n",
    "***\n",
    "\n",
    "Credit for GPT2-RGA code used in this colab goes out @ Sashmark97 https://github.com/Sashmark97/midigen and @ Damon Gwinn https://github.com/gwinndr/MusicTransformer-Pytorch\n",
    "\n",
    "***\n",
    "\n",
    "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
    "\n",
    "***\n",
    "\n",
    "#### Project Los Angeles\n",
    "\n",
    "#### Tegridy Code 2021\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fa0a611c-1803-42ae-bdf6-a49b5a4e781b",
     "kernelId": ""
    },
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "# (Setup Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "editing": false,
     "id": "39411b40-9e39-416e-8fe4-d40f733e7956",
     "kernelId": ""
    },
    "id": "lw-4aqV3sKQG"
   },
   "outputs": [],
   "source": [
    "#@title nvidia-smi gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "editing": false,
     "id": "a1a45a91-d909-4fd4-b67a-5e16b971d179",
     "kernelId": ""
    },
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "!git clone https://github.com/asigalov61/tegridy-tools\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "editing": false,
     "id": "b8207b76-9514-4c07-95db-95a4742e52c5",
     "kernelId": ""
    },
    "id": "z7n9vnKmug1J"
   },
   "outputs": [],
   "source": [
    "#@title Import all needed modules\n",
    "\n",
    "print('Loading needed modules. Please wait...')\n",
    "import os\n",
    "from datetime import datetime\n",
    "import secrets\n",
    "import copy\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists('/notebooks/Dataset'):\n",
    "    os.makedirs('/notebooks/Dataset')\n",
    "\n",
    "print('Loading TMIDIX module...')\n",
    "os.chdir('/notebooks/tegridy-tools/tegridy-tools')\n",
    "import TMIDIX\n",
    "\n",
    "os.chdir('/notebooks/tegridy-tools/tegridy-tools')\n",
    "from GPT2RGAX import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('/notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObPxlEutsQBj"
   },
   "source": [
    "# (FROM SCRATCH) Download and process MIDI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "ffbb7a2a-d91a-477f-ac89-56d77d6cdf42",
     "kernelId": ""
    },
    "id": "snIZ3xKPsPgB"
   },
   "outputs": [],
   "source": [
    "#@title Download Endless Violin Carousel MIDI dataset (Recommended)\n",
    "\n",
    "#@markdown Piano Violin Duo\n",
    "\n",
    "#@markdown Works best stand-alone/as-is for the optimal results\n",
    "%cd /notebooks/Dataset/\n",
    "\n",
    "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Endless-Violin-Carousel-CC-BY-NC-SA.zip'\n",
    "!unzip -j '/notebooks/Dataset/Endless-Violin-Carousel-CC-BY-NC-SA.zip'\n",
    "!rm '/notebooks/Dataset/Endless-Violin-Carousel-CC-BY-NC-SA.zip'\n",
    "\n",
    "%cd /notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "ed07b44f-07fe-45fb-a64f-adba8df1bdcb",
     "kernelId": ""
    },
    "id": "on7sgKEP3Yc8"
   },
   "outputs": [],
   "source": [
    "#@title Process MIDIs to special MIDI dataset with Tegridy MIDI Processor\n",
    "\n",
    "#@markdown IMPORTANT NOTES:\n",
    "\n",
    "#@markdown 1) Best results are achieved with the single-track, single-channel, single-instrument MIDI 0 files with plain English names (avoid special or sys/foreign chars)\n",
    "\n",
    "#@markdown 2) MIDI Channel = -1 means all MIDI channels except the drums. MIDI Channel = 16 means all channels will be processed. Otherwise, only single indicated MIDI channel will be processed\n",
    "\n",
    "desired_dataset_name = \"Quantum-Music-Dataset\" #@param {type:\"string\"}\n",
    "file_name_to_output_dataset_to = \"/notebooks/Quantum-Music-Dataset\" #@param {type:\"string\"}\n",
    "desired_MIDI_channel_to_process = -1 #@param {type:\"slider\", min:-1, max:16, step:1}\n",
    "sorted_or_random_file_loading_order = False #@param {type:\"boolean\"}\n",
    "encode_velocities = True #@param {type:\"boolean\"}\n",
    "encode_MIDI_channels = True #@param {type:\"boolean\"}\n",
    "add_transposed_dataset_by_this_many_pitches = 0 #@param {type:\"slider\", min:-12, max:12, step:1}\n",
    "add_transposed_and_flipped_dataset = False #@param {type:\"boolean\"}\n",
    "chordify_input_MIDIs = False #@param {type:\"boolean\"}\n",
    "melody_conditioned_chords = False #@param {type:\"boolean\"}\n",
    "melody_pitch_baseline = 60 #@param {type:\"slider\", min:0, max:127, step:1}\n",
    "time_denominator = 1 #@param {type:\"slider\", min:1, max:50, step:1}\n",
    "transform_to_pitch = 0 #@param {type:\"slider\", min:0, max:127, step:1}\n",
    "perfect_timings = True #@param {type:\"boolean\"}\n",
    "MuseNet_encoding = True #@param {type:\"boolean\"}\n",
    "chars_encoding_offset = 0 #@param {type:\"number\"}\n",
    "\n",
    "print('TMIDI Optimus MIDI Processor')\n",
    "print('Starting up...')\n",
    "###########\n",
    "\n",
    "average_note_pitch = 0\n",
    "min_note = 127\n",
    "max_note = 0\n",
    "\n",
    "files_count = 0\n",
    "\n",
    "gfiles = 0\n",
    "\n",
    "chords_list_f = []\n",
    "melody_list_f = []\n",
    "\n",
    "chords_list = []\n",
    "chords_count = 0\n",
    "\n",
    "melody_chords = []\n",
    "melody_count = 0\n",
    "\n",
    "TXT_String = ''\n",
    "\n",
    "TXT = ''\n",
    "melody = []\n",
    "chords = []\n",
    "INTS_f = []\n",
    "\n",
    "flist = []\n",
    "\n",
    "###########\n",
    "\n",
    "print('Loading MIDI files...')\n",
    "print('This may take a while on a large dataset in particular.')\n",
    "\n",
    "dataset_addr = \"/notebooks/Dataset/\"\n",
    "os.chdir(dataset_addr)\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "\n",
    "if filez == []:\n",
    "  print('Could not find any MIDI files. Please check Dataset dir...')\n",
    "  print('=' * 70)\n",
    "\n",
    "if sorted_or_random_file_loading_order:\n",
    "  print('Sorting files...')\n",
    "  filez.sort()\n",
    "  print('Done!')\n",
    "  print('=' * 70)\n",
    "\n",
    "else:\n",
    "  random.shuffle(filez)\n",
    "\n",
    "# Stamping the dataset info\n",
    "print('Stamping the dataset info...')\n",
    "\n",
    "TXT_String += 'DATASET=' + str(desired_dataset_name) + chr(10)\n",
    "TXT_String += 'CREATED_ON=' + str(datetime.now()).replace(' ', '-').replace(':', '-').replace('.', '-') + chr(10)\n",
    "\n",
    "TXT_String += 'CHARS_ENCODING_OFFSET=' + str(chars_encoding_offset) + chr(10)\n",
    "TXT_String += 'TIME_DENOMINATOR=' + str(time_denominator) + chr(10)\n",
    "TXT_String += 'TRANSFORM=' + str(transform_to_pitch) + chr(10)\n",
    "TXT_String += 'PERFECT_TIMINGS=' + str(perfect_timings) + chr(10)\n",
    "TXT_String += 'MUSENET_ENCODING=' + str(MuseNet_encoding) + chr(10)\n",
    "TXT_String += 'TRANSPOSED_BY=' + str(add_transposed_dataset_by_this_many_pitches) + chr(10)\n",
    "TXT_String += 'TRANSPOSED_AND_FLIPPED=' + str(add_transposed_and_flipped_dataset) + chr(10)\n",
    "\n",
    "TXT_String += 'LEGEND=STA-DUR-PTC'\n",
    "if encode_velocities:\n",
    "  TXT_String += '-VEL'\n",
    "if encode_MIDI_channels:\n",
    "  TXT_String += '-CHA'\n",
    "TXT_String += chr(10)\n",
    "\n",
    "print('Processing MIDI files. Please wait...')\n",
    "for f in tqdm(filez):\n",
    "  try:\n",
    "    fn = os.path.basename(f)\n",
    "    fn1 = fn.split('.')[0]\n",
    "\n",
    "    files_count += 1\n",
    "    TXT, melody, chords, bass_melody, karaokez, INTS, aux1, aux2 = TMIDIX.Optimus_MIDI_TXT_Processor(f, chordify_TXT=chordify_input_MIDIs, output_MIDI_channels=encode_MIDI_channels, char_offset=chars_encoding_offset, dataset_MIDI_events_time_denominator=time_denominator, output_velocity=encode_velocities, MIDI_channel=desired_MIDI_channel_to_process, MIDI_patch=range(0, 127), melody_conditioned_encoding=melody_conditioned_chords, melody_pitch_baseline=melody_pitch_baseline, perfect_timings=perfect_timings, musenet_encoding=MuseNet_encoding, transform=transform_to_pitch)\n",
    "    TXT_String += TXT\n",
    "    melody_list_f += melody\n",
    "    chords_list_f.append(chords)\n",
    "    INTS_f.append(INTS)\n",
    "    flist.append([f, fn1])\n",
    "    gfiles += 1\n",
    "\n",
    "    if add_transposed_dataset_by_this_many_pitches != 0:\n",
    "\n",
    "      TXT, melody, chords, bass_melody, karaokez, INTS, aux1, aux2 = TMIDIX.Optimus_MIDI_TXT_Processor(f, chordify_TXT=chordify_input_MIDIs, output_MIDI_channels=encode_MIDI_channels, char_offset=chars_encoding_offset, dataset_MIDI_events_time_denominator=time_denominator, output_velocity=encode_velocities, MIDI_channel=desired_MIDI_channel_to_process, transpose_by=add_transposed_dataset_by_this_many_pitches, MIDI_patch=range(0, 127), melody_conditioned_encoding=melody_conditioned_chords, melody_pitch_baseline=melody_pitch_baseline, perfect_timings=perfect_timings, musenet_encoding=MuseNet_encoding, transform=transform_to_pitch)\n",
    "      TXT_String += TXT\n",
    "      melody_list_f += melody\n",
    "      chords_list_f.append(chords)\n",
    "      INTS_f.append(INTS)\n",
    "      gfiles += 1\n",
    "\n",
    "    if add_transposed_and_flipped_dataset == True:\n",
    "\n",
    "      TXT, melody, chords, bass_melody, karaokez, INTS, aux1, aux2 = TMIDIX.Optimus_MIDI_TXT_Processor(f, chordify_TXT=chordify_input_MIDIs, output_MIDI_channels=encode_MIDI_channels, char_offset=chars_encoding_offset, dataset_MIDI_events_time_denominator=time_denominator, output_velocity=encode_velocities, MIDI_channel=desired_MIDI_channel_to_process, transpose_by=-12, MIDI_patch=range(0, 127), flip=True, melody_conditioned_encoding=melody_conditioned_chords, melody_pitch_baseline=melody_pitch_baseline, perfect_timings=perfect_timings, musenet_encoding=MuseNet_encoding, transform=transform_to_pitch)\n",
    "      TXT_String += TXT\n",
    "      melody_list_f += melody\n",
    "      chords_list_f += chords\n",
    "      INTS_f.append(INTS)\n",
    "      gfiles += 1\n",
    "\n",
    "  except KeyboardInterrupt:\n",
    "    print('Saving current progress and quitting...')\n",
    "    break  \n",
    "  \n",
    "  except:\n",
    "    print('Bad MIDI:', f)\n",
    "    continue\n",
    "\n",
    "TXT_String += 'TOTAL_SONGS_IN_DATASET=' + str(gfiles)\n",
    "\n",
    "try:\n",
    "  print('Task complete :)')\n",
    "  print('==================================================')\n",
    "  if add_transposed_dataset_by_this_many_pitches != 0:\n",
    "    print('NOTE: Transposed dataset was added per users request.')\n",
    "    print('==================================================')\n",
    "  if add_transposed_and_flipped_dataset == True:\n",
    "    print('NOTE: Flipped dataset was added per users request.')  \n",
    "    print('==================================================')\n",
    "  print('Number of processed dataset MIDI files:', files_count)\n",
    "  print('Number of MIDI chords recorded:', len(chords_list_f))\n",
    "  print('First chord event:', chords_list_f[0], 'Last chord event:', chords_list_f[-1]) \n",
    "  print('Number of recorded melody events:', len(melody_list_f))\n",
    "  print('First melody event:', melody_list_f[0], 'Last Melody event:', melody_list_f[-1])\n",
    "  print('Total number of MIDI events recorded:', len(chords_list_f) + len(melody_list_f))\n",
    "  print('==================================================')\n",
    "\n",
    "  # Writing dataset to TXT file\n",
    "  with open(file_name_to_output_dataset_to + '.txt', 'wb') as f:\n",
    "    f.write(TXT_String.encode('utf-8', 'replace'))\n",
    "    f.close\n",
    "\n",
    "  # Dataset\n",
    "  MusicDataset = [chords_list_f, melody_list_f, INTS_f]\n",
    "\n",
    "  # Writing dataset to pickle file\n",
    "  TMIDIX.Tegridy_Any_Pickle_File_Writer(MusicDataset, file_name_to_output_dataset_to)\n",
    "\n",
    "except:\n",
    "  print('=' * 70)\n",
    "  print('IO Error!')\n",
    "  print('Please check that Dataset dir is not empty/check other IO code.')\n",
    "  print('=' * 70)\n",
    "  print('Shutting down...')\n",
    "  print('=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "0826f622-2edc-4f09-9a01-58df049738d4",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "INTS_f1 = []\n",
    "\n",
    "\n",
    "for chords_list in tqdm(chords_list_f):\n",
    "    INTS_f1.append([-1, -1, -1]) # Intro\n",
    "    pe = chords_list[0]\n",
    "    for i in chords_list:\n",
    "\n",
    "        INTS_f1.append([int(abs(i[1]-pe[1])/ 10), int(i[2] / 10), i[4] ])\n",
    "        \n",
    "        if chords_list.index(i) == len(chords_list)-50:\n",
    "            INTS_f1.append([-2, -2, -2]) # Outro\n",
    "        \n",
    "        \n",
    "        pe = i\n",
    "    INTS_f1.append([-3, -3, -3]) # End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "53252e52-5e68-4e60-8e4d-a584667749a4",
     "kernelId": ""
    },
    "id": "lT0TyqUnpxu_"
   },
   "outputs": [],
   "source": [
    "#@title Load processed INTs datasets\n",
    "number_of_batches = 16 #@param {type:\"slider\", min:2, max:32, step:2}\n",
    "n_workers = 6\n",
    "\n",
    "print('=' * 50)\n",
    "print('Prepping INTs datasets...')\n",
    "\n",
    "\n",
    "train_data1 = []\n",
    "for i in INTS_f1:\n",
    "  if max(i) < 256 and min(i) >= 0:\n",
    "\n",
    "      if i[0] < 16:\n",
    "        train_data1.extend([i[0]])\n",
    "      else:\n",
    "        train_data1.extend([16, i[0]-16])\n",
    "       \n",
    "      train_data1.extend([256+i[2], 512+i[1]-4 ])\n",
    "  \n",
    "  if max(i) == -1 and min(i) == -1: # Intro\n",
    "      train_data1.extend([256+512-3])\n",
    "  \n",
    "  if max(i) == -2 and min(i) == -2: # Outro\n",
    "      train_data1.extend([256+512-2])\n",
    "  \n",
    "  if max(i) == -3 and min(i) == -3: # End\n",
    "      train_data1.extend([256+512-1])\n",
    "\n",
    "train_data = train_data1[:int(len(train_data1) / 3)]\n",
    "\n",
    "val_dataset = train_data[:int(len(train_data) * 0.03)]\n",
    "test_dataset = train_data[:int(len(train_data) * 0.03)]\n",
    "\n",
    "train_list = train_data\n",
    "val_list = val_dataset\n",
    "test_list = []\n",
    "print('=' * 50)\n",
    "\n",
    "print('Processing INTs datasets...')\n",
    "train_dataset = EPianoDataset(train_list, max_seq, random_seq)\n",
    "val_dataset = EPianoDataset(val_list, max_seq)\n",
    "test_dataset = EPianoDataset(test_list, max_seq)\n",
    "print('=' * 50)\n",
    "\n",
    "print('Loading INTs datasets...')\n",
    "batch_size = number_of_batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "print('=' * 50)\n",
    "\n",
    "print('Total INTs in the dataset', len(train_data))\n",
    "print('Total unique INTs in the dataset', len(set(train_data)))\n",
    "print('Max INT in the dataset', max(train_data))\n",
    "print('Min INT in the dataset', min(train_data))\n",
    "print('=' * 50)\n",
    "\n",
    "print('Checking datasets shapes...')\n",
    "print('=' * 50)\n",
    "\n",
    "print('Train loader')\n",
    "for x, tgt in train_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "print('=' * 50)\n",
    "\n",
    "print('Validation loader')\n",
    "for x, tgt in val_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "print('=' * 50)\n",
    "\n",
    "print('Test loader')\n",
    "for x, tgt in test_loader:\n",
    "    print(f'X shape: {x.shape}')\n",
    "    print(f'Target shape: {tgt.shape}')\n",
    "    break\n",
    "print('=' * 50)\n",
    "\n",
    "print('Done! Enjoy! :)')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the resulting INTs dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "708f16d3-1747-4e72-bcc9-7504cdd963d4",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 6,
     "id": "dd411e56-532f-47dd-8283-ecb57126a3ae",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "out = train_data[:10000]\n",
    "if len(out) != 0:\n",
    "  song = []\n",
    "  song = out\n",
    "  song_f = []\n",
    "  time = 0\n",
    "  pitch = 0\n",
    "  duration = 0\n",
    "  for s in song:\n",
    "    if s >= 0 and s <= 256:\n",
    "        time += s\n",
    "    if s >= 256 and s < 512:\n",
    "        pitch = s-256\n",
    "    if s >= 512 and s < 256+512-4:\n",
    "        duration = s-512\n",
    "        song_f.append(['note', (abs(time))*10, (duration*10), 0, pitch, pitch ])\n",
    "    \n",
    "  detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Quantum Music',  \n",
    "                                                        output_file_name = '/notebooks/Quantum-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles', \n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "  print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkVqviDzJOrv"
   },
   "source": [
    "# (TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9CBW8xYupH8"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "4aa21407-a3e9-4ed2-9bf1-83c295482b8a",
     "kernelId": ""
    },
    "id": "2moo7uUmpxvC"
   },
   "outputs": [],
   "source": [
    "#@title Train\n",
    "config = GPTConfig(VOCAB_SIZE, \n",
    "                   max_seq,\n",
    "                   dim_feedforward=dim_feedforward,\n",
    "                   n_layer=6, \n",
    "                   n_head=8, \n",
    "                   n_embd=512,\n",
    "                   enable_rpr=True,\n",
    "                   er_len=max_seq)\n",
    "model = GPT(config).to(get_device())\n",
    "\n",
    "#=====\n",
    "\n",
    "init_step = 0\n",
    "lr = LR_DEFAULT_START\n",
    "lr_stepper = LrStepTracker(d_model, SCHEDULER_WARMUP_STEPS, init_step)\n",
    "eval_loss_func = nn.CrossEntropyLoss(ignore_index=TOKEN_PAD)\n",
    "train_loss_func = eval_loss_func\n",
    "\n",
    "opt = Adam(model.parameters(), lr=lr, betas=(ADAM_BETA_1, ADAM_BETA_2), eps=ADAM_EPSILON)\n",
    "lr_scheduler = LambdaLR(opt, lr_stepper.step)\n",
    "\n",
    "\n",
    "#===\n",
    "\n",
    "best_eval_acc        = 0.0\n",
    "best_eval_acc_epoch  = -1\n",
    "best_eval_loss       = float(\"inf\")\n",
    "best_eval_loss_epoch = -1\n",
    "best_acc_file = '/notebooks/gpt2_rpr_acc.pth'\n",
    "best_loss_file = '/notebooks/gpt2_rpr_loss.pth'\n",
    "loss_train, loss_val, acc_val = [], [], []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    new_best = False\n",
    "    \n",
    "    loss = train(epoch+1, model, train_loader, train_loss_func, opt, lr_scheduler, num_iters=-1)\n",
    "    loss_train.append(loss)\n",
    "    \n",
    "    eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)\n",
    "    loss_val.append(eval_loss)\n",
    "    acc_val.append(eval_acc)\n",
    "    \n",
    "    if(eval_acc > best_eval_acc):\n",
    "        best_eval_acc = eval_acc\n",
    "        best_eval_acc_epoch  = epoch+1\n",
    "        torch.save(model.state_dict(), best_acc_file)\n",
    "        new_best = True\n",
    "\n",
    "    if(eval_loss < best_eval_loss):\n",
    "        best_eval_loss       = eval_loss\n",
    "        best_eval_loss_epoch = epoch+1\n",
    "        torch.save(model.state_dict(), best_loss_file)\n",
    "        new_best = True\n",
    "    \n",
    "    if(new_best):\n",
    "        print(\"Best eval acc epoch:\", best_eval_acc_epoch)\n",
    "        print(\"Best eval acc:\", best_eval_acc)\n",
    "        print(\"\")\n",
    "        print(\"Best eval loss epoch:\", best_eval_loss_epoch)\n",
    "        print(\"Best eval loss:\", best_eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss, eval_acc = eval_model(model, val_loader, eval_loss_func, num_iters=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "0e338550-f170-44a6-9479-ba0ddbc64608",
     "kernelId": ""
    },
    "id": "NNqmcFdRyC2M"
   },
   "outputs": [],
   "source": [
    "#@title Plot resulting training loss graph\n",
    "\n",
    "tr_loss_list = [item for sublist in loss_train for item in sublist]\n",
    "plt.plot([i for i in range(len(tr_loss_list))] ,tr_loss_list, 'b')\n",
    "plt.savefig('/notebooks/training-loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdKFoeke9L7H"
   },
   "source": [
    "# (SAVE/LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "73bea62d-084b-4f9a-9e55-2b34a932a7a4",
     "kernelId": ""
    },
    "id": "gqyDatHC9X1z"
   },
   "outputs": [],
   "source": [
    "#@title Save the model\n",
    "\n",
    "print('Saving the model...')\n",
    "full_path_to_model_checkpoint = \"/notebooks/Quantum-Music-Trained-Model-6.pth\" #@param {type:\"string\"}\n",
    "torch.save(model.state_dict(), full_path_to_model_checkpoint)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "c83edd89-9a36-430a-9fa7-3a967417c88e",
     "kernelId": ""
    },
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [],
   "source": [
    "#@title Load/Reload the model\n",
    "full_path_to_model_checkpoint = \"/notebooks/Quantum-Music-Trained-Model-6.pth\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading the model...')\n",
    "config = GPTConfig(256+512+2, \n",
    "                   max_seq,\n",
    "                   dim_feedforward=dim_feedforward,\n",
    "                   n_layer=6, \n",
    "                   n_head=8, \n",
    "                   n_embd=512,\n",
    "                   enable_rpr=True,\n",
    "                   er_len=max_seq)\n",
    "\n",
    "model = GPT(config).to(get_device())\n",
    "\n",
    "model.load_state_dict(torch.load(full_path_to_model_checkpoint))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom MIDI option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "5f771604-39e7-431d-b1dd-86d7437b8872",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "data = TMIDIX.Optimus_MIDI_TXT_Processor('/notebooks/seed97-super.mid', \n",
    "                                         dataset_MIDI_events_time_denominator=10, \n",
    "                                         perfect_timings=True, \n",
    "                                         musenet_encoding=True, \n",
    "                                         char_offset=0, \n",
    "                                         MIDI_channel=-1, \n",
    "                                         MIDI_patch=range(0, 127)\n",
    "                                        )\n",
    "\n",
    "SONG = data[5]\n",
    "inputs = []\n",
    "for i in SONG:\n",
    "    if max(i) < 256 and max(i) >= 0:\n",
    "        if i[0] < 16:\n",
    "            inputs.extend([i[0]])\n",
    "        else:\n",
    "            \n",
    "            inputs.extend([16, i[0]-16])\n",
    "        \n",
    "        inputs.extend([256+i[3], 512+i[1] ])            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX1_5y5Fu8AH"
   },
   "source": [
    "# (GENERATE MUSIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "97793d01-6a74-4e34-be95-ea337277b38d",
     "kernelId": ""
    },
    "id": "M_K93hWWv2Yx"
   },
   "outputs": [],
   "source": [
    "#@title Generate and download a MIDI file\n",
    "\n",
    "number_of_tokens_to_generate = 1024 #@param {type:\"slider\", min:8, max:1024, step:8}\n",
    "use_random_primer = False #@param {type:\"boolean\"}\n",
    "start_with_zero_token = False #@param {type:\"boolean\"}\n",
    "number_of_ticks_per_quarter = 500 #@param {type:\"slider\", min:50, max:1000, step:50}\n",
    "dataset_time_denominator = 10\n",
    "melody_conditioned_encoding = False\n",
    "encoding_has_MIDI_channels = False \n",
    "encoding_has_velocities = False\n",
    "simulate_velocity = True #@param {type:\"boolean\"}\n",
    "save_only_first_composition = True\n",
    "chars_encoding_offset_used_for_dataset = 33\n",
    "\n",
    "fname = '/notebooks/Quantum-Music-Composition'\n",
    "\n",
    "print('Quantum Music Model Generator')\n",
    "\n",
    "output_signature = 'Quantum Music'\n",
    "song_name = 'RGA Composition'\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if use_random_primer:\n",
    "  sequence = [random.randint(10, 387) for i in range(64)]\n",
    "  idx = secrets.randbelow(len(sequence))\n",
    "  rand_seq = model.generate(torch.Tensor(sequence[idx:idx+120]), target_seq_length=number_of_tokens_to_generate)\n",
    "  out = rand_seq[0].cpu().numpy().tolist()\n",
    "\n",
    "else:\n",
    "  out = []\n",
    "  \n",
    "  try:\n",
    "    if start_with_zero_token:\n",
    "      sequence = inputs[-512:] #[256+512 - 2, 0]# inputs[-512:]\n",
    "      rand_seq = model.generate(torch.Tensor(sequence), target_seq_length=number_of_tokens_to_generate, stop_token=256+512)\n",
    "      out = rand_seq[0].cpu().numpy().tolist()\n",
    "    else:\n",
    "      idx = secrets.randbelow(len(train_data))\n",
    "      sequence = train_data[idx:idx+512]\n",
    "      rand_seq = model.generate(torch.Tensor(sequence), target_seq_length=number_of_tokens_to_generate, stop_token=256+512)\n",
    "      out = rand_seq[0].cpu().numpy().tolist()\n",
    "  \n",
    "  except:\n",
    "    print('=' * 50)\n",
    "    print('Error! Try random priming instead!')\n",
    "    print('Shutting down...')\n",
    "    print('=' * 50)\n",
    "\n",
    "if len(out) != 0:\n",
    "  song = []\n",
    "  song = out\n",
    "  song_f = []\n",
    "  time = 0\n",
    "  pitch = 0\n",
    "  duration = 0\n",
    "  once = True\n",
    "  for s in song:\n",
    "    if s >= 0 and s < 256:\n",
    "        time += s\n",
    "    if s >= 256 and s < 512:\n",
    "        pitch = s-256\n",
    "    if s >= 512 and s < 256+512-4:\n",
    "        duration = s-512\n",
    "        song_f.append(['note', (abs(time))*10, (duration*10), 0, pitch, pitch ])\n",
    "    \n",
    "    if song.index(s) >= len(sequence) and once:\n",
    "        song_f.append(['text_event', abs(time) * 10, 'Continuation Start Here'])\n",
    "        once = False\n",
    "    \n",
    "  detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Quantum Music',  \n",
    "                                                        output_file_name = '/notebooks/Quantum-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles', \n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "  \n",
    "  print('Done!')\n",
    "\n",
    "\n",
    "  #print('Downloading your composition now...')\n",
    "  #from google.colab import files\n",
    "  #files.download(fname + '.mid')\n",
    "\n",
    "  print('=' * 70)\n",
    "  print('Detailed MIDI stats:')\n",
    "  for key, value in detailed_stats.items():\n",
    "        print('=' * 70)\n",
    "        print(key, '|', value)\n",
    "\n",
    "  print('=' * 70)\n",
    "\n",
    "else:\n",
    "  print('Models output is empty! Check the code...')\n",
    "  print('Shutting down...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[-64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "id": "c8149763-2e09-4fcf-9823-85ca778b9e80",
     "kernelId": ""
    },
    "id": "STtGgBsf4-tA"
   },
   "outputs": [],
   "source": [
    "#@title Auto-Regressive Generator\n",
    "\n",
    "#@markdown NOTE: You much generate a seed composition first or it is not going to start\n",
    "\n",
    "number_of_cycles_to_run = 5 #@param {type:\"slider\", min:1, max:50, step:1}\n",
    "number_of_prime_tokens = 128 #@param {type:\"slider\", min:64, max:256, step:64}\n",
    "\n",
    "print('=' * 70)\n",
    "print('Quantum Music Auto-Regressive Model Generator')\n",
    "print('=' * 70)\n",
    "print('Starting up...')\n",
    "print('=' * 70)\n",
    "print('Prime length:', len(out))\n",
    "print('Prime tokens:', number_of_prime_tokens)\n",
    "print('Prime input sequence', out[-8:])\n",
    "\n",
    "if len(out) != 0:\n",
    "  print('=' * 70)\n",
    "  out_all = []\n",
    "  out_all.append(out)\n",
    "  for i in tqdm(range(number_of_cycles_to_run)):\n",
    "      rand_seq1 = model.generate(torch.Tensor(out[-number_of_prime_tokens:]), target_seq_length=1024, stop_token=256+512)\n",
    "      out1 = rand_seq1[0].cpu().numpy().tolist()\n",
    "      out_all.append(out1[number_of_prime_tokens:])\n",
    "      out = out1[number_of_prime_tokens:]\n",
    "      \n",
    "      print(chr(10))\n",
    "      print('=' * 70)\n",
    "      print('Block number:', i+1)\n",
    "      print('Composition length so far:', (i+1) * 1024, 'notes')\n",
    "      print('=' * 70)\n",
    "\n",
    "  print('Done!' * 70)\n",
    "  print('Total blocks:', i+1)\n",
    "  print('Final omposition length:', (i+1) * 1024, 'notes')\n",
    "  print('=' * 70)\n",
    "  \n",
    "  out2 = []\n",
    "  for o in out_all:\n",
    "    out2.extend(o)\n",
    "\n",
    "  if len(out2) != 0:\n",
    "      song = []\n",
    "      song = out2\n",
    "      song_f = []\n",
    "      time = 0\n",
    "      pitch = 0\n",
    "      duration = 0\n",
    "      once = True\n",
    "      for s in song:\n",
    "        if s >= 0 and s < 256:\n",
    "            time += s\n",
    "        if s >= 256 and s < 512:\n",
    "            pitch = s-256\n",
    "        if s >= 512 and s < 256+512-4:\n",
    "            duration = s-512\n",
    "            song_f.append(['note', (abs(time))*10, (duration*10), 0, pitch, pitch ])\n",
    "\n",
    "      detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                            output_signature = 'Quantum Music',  \n",
    "                                                            output_file_name = '/notebooks/Quantum-Music-Composition', \n",
    "                                                            track_name='Project Los Angeles', \n",
    "                                                            number_of_ticks_per_quarter=500)\n",
    "\n",
    "      print('Done!')\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "  print('=' * 70)\n",
    "  print('INPUT ERROR !!!')\n",
    "  print('Prime sequence is empty...')\n",
    "  print('Please generate prime sequence and retry')\n",
    "\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzCMd94Tu_gz"
   },
   "source": [
    "# Congrats! You did it! :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
