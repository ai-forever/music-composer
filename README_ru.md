# Music Composer
Данный репозиторий посвящен синтезу символьной музыки в MIDI формате с помощью модели Music Transformer. В репозитории можно найти демонстрационный ноутбук для генерации на GPU инстансе Google Colab, код подготовки данных и обучения модели.

## Оглавление
1. [Демонстрационный ноутбук](#демонстрационный-ноутбук)
2. [Код модели](#код-модели)
3. [Данные](#данные)
4. [Обучение](#обучение)


## Демонстрационный ноутбук

Jupyter Notebook можно открыть на Colab нажав на кнопку:

[![Open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai-forever/music-composer/blob/main/src/Music_Composer_Demo_Colab_ru.ipynb)

В нем производится разворачивание окружения, подгрузка кода и весов для синтеза. Параметры генерации задаются в панели управления генерацией, а прослушать и скачать результаты можно в последней ячейке. 

❗При запуске убедитесь в том, что используется GPU инстанс. Можно синтезировать и на CPU, но это занимает ощутимо больше времени.

## Код модели
Расположен в [папке](https://github.com/ai-forever/music-composer/tree/main/src/lib/model). 
Состоит из трех осовных частей:
- Positional encoding - обычное позиционное кодирование для трансформерных моделей
- Relative Positional Representation - модуль с реализацией Relative Attention
- Transformer - сама модель трансформер  

Код модели и relative attention взят из [репозитория](https://github.com/gwinndr/MusicTransformer-Pytorch).

## Данные
Для демонстрации скрипта энкодинга мы предоставляем несколько MIDI файлов из нашей обучающей выборки. Они находятся в папке src/test_dataset и разбиты по папкам на жанры. В каждой папке по одному файлу для проверки. Запустить подготовку закодированных в event-based формате версий этих файлов можно с помощью команды:
```python encode_dataset.py```

Папка с исходными MIDI и папка для результатов задаются внутри скрипта через переменные `DATA_DIR`, `OUTPUT_DIR`. Файлы датасетов с путями до файлов будут созданы в `DS_FILE_PATH`. Список жанров задается через `GENRES`, а максимальная длина записи в event токенах - `MAX_LEN`.

Для демонстрации мы также предоставляем результат работы данной команды в папке encoded_dataset. В нем находятся тензоры с MIDI, переведенными в event-based формат. Их можно загрузить с помощью стандартного `torch.load(file_path)`
В качестве общедоступных MIDI для обучения можно использовать датасеты:
[MAESTRO Dataset](https://magenta.tensorflow.org/datasets/maestro)
[Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/)
[GiantMIDI-Piano Dataset](https://github.com/bytedance/GiantMIDI-Piano)

Есть еще один способ получения MIDI файлов - транскрибирование волновых файлов с музыкой. В этом может помочь подход наподобие [Onset-frames](https://magenta.tensorflow.org/onsets-frames).
В качестве музыки для транскрибирования можно использовать например [Free Music Archive](https://github.com/mdeff/fma).  
❗Для транскрибирования могут потребоваться значительные ресурсы, однако именно это позволит обойти основное ограничение текущих моделей генерации символьной музыки - отсутствие крупных корпусов с нотами.  
❗ После транскрибирования рекомендуется проанализировать результаты и отфильтровать плохие записи.
## Обучение
Скрипт для обучения модели на подготовленных данных можно запустить с помощью:
```python train.py```
Параметры обучения задаются внутри скрипта в переменной params. Позднее в данном разделе будет дано описание каждого из параметров.
